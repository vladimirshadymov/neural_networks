{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda install pytorch torchvision -c soumith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors behave almost exactly the same way in PyTorch as they do in\n",
    "Torch.\n",
    "\n",
    "Create a tensor of size (5 x 7) with uninitialized memory:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a tensor randomized with a normal distribution with mean=0, var=1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7292, -0.6780, -0.8854, -0.6688, -0.2979, -0.2549, -0.4290],\n",
      "        [-0.2959, -2.1381,  0.3735, -1.6117, -0.5397, -0.8251, -0.0944],\n",
      "        [-0.3142, -1.3450, -0.9411,  0.2496,  0.2823, -0.2959, -0.0048],\n",
      "        [-0.1951, -0.8663, -0.2173,  2.1329, -0.6608,  0.5312, -0.9109],\n",
      "        [ 0.2669,  0.5877,  0.9939,  0.0922, -1.3421,  0.8412, -0.9142]])\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports the same operations</p></div>\n",
    "\n",
    "Inplace / Out-of-place\n",
    "----------------------\n",
    "\n",
    "The first difference is that ALL operations on the tensor that operate\n",
    "in-place on it will have an ``_`` postfix. For example, ``add`` is the\n",
    "out-of-place version, and ``add_`` is the in-place version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]) tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]])\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "# a has now been filled with the value 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a is still filled with 3.5\n",
    "# new tensor b is returned with values 3.5 + 4.0 = 7.5\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations like ``narrow`` do not have in-place versions, and\n",
    "hence, ``.narrow_`` does not exist. Similarly, some operations like\n",
    "``fill_`` do not have an out-of-place version, so ``.fill`` does not\n",
    "exist.\n",
    "\n",
    "Zero Indexing\n",
    "-------------\n",
    "\n",
    "Another difference is that Tensors are zero-indexed. (In lua, tensors are\n",
    "one-indexed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0, 3]  # select 1st row, 4th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be also indexed with Python's slicing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, 3:5]  # selects all rows, 4th column and  5th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No camel casing\n",
    "---------------\n",
    "\n",
    "The next small difference is that all functions are now NOT camelCase\n",
    "anymore. For example ``indexAdd`` is now called ``index_add_``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.]])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.LongTensor([4, 0]), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Bridge\n",
    "------------\n",
    "\n",
    "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
    "The torch Tensor and numpy array will share their underlying memory\n",
    "locations, and changing one will change the other.\n",
    "\n",
    "Converting torch Tensor to numpy Array\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) \t# see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy Array to torch Tensor\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to\n",
    "NumPy and back.\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "CUDA Tensors are nice and easy in pytorch, and transfering a CUDA tensor\n",
    "from the CPU to GPU will retain its underlying type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # creates a LongTensor and transfers it\n",
    "    # to GPU as torch.cuda.LongTensor\n",
    "    a = torch.LongTensor(10).fill_(3).cuda()\n",
    "    print(type(a))\n",
    "    b = a.cpu()\n",
    "    # transfers it to CPU, back to\n",
    "    # being a torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6321, 0.7192, 0.2566],\n",
      "        [0.5308, 0.1696, 0.3069],\n",
      "        [0.8424, 0.9149, 0.4901]])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6321, 0.7192, 0.2566, 0.0000],\n",
      "        [0.5308, 0.1696, 0.3069, 0.0000],\n",
      "        [0.8424, 0.9149, 0.4901, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.5283, 0.8030, 0.8572],\n",
      "        [0.9450, 0.7898, 1.3008],\n",
      "        [1.4434, 1.3736, 0.5900]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "t4d = torch.rand(3, 3)\n",
    "t4d2 = torch.rand(3, 3)\n",
    "print(t4d)\n",
    "p1d = (0, 1, 2,2 ) # pad last dim by 1 on each side\n",
    "out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
    "print(out)\n",
    "print(t4d+t4d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
